{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec4abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d5b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1 = '/home/data_backup/data_bu/eva_dataset/materialForEEVA/images/session2/client'\n",
    "# d2 = '/home/data_backup/data_bu/eva_dataset/materialForEEVA/images/session2/counselor'\n",
    "# # iterate over files in\n",
    "# # that directory\n",
    "\n",
    "# data_in = []\n",
    "# data_out = []\n",
    "# for filename in sorted(os.listdir(d1)):\n",
    "#     fn = os.path.join(d1, filename)\n",
    "#     if os.path.isfile(fn):\n",
    "#         with open(fn) as f:\n",
    "#             lines = f.readlines()\n",
    "#             aline = []\n",
    "#             for x in lines:\n",
    "#                 aline.append([int(y) for y in x.rstrip('\\n').split(\" \")[-2:]])\n",
    "#             data_in.append(aline)\n",
    "#         f.close()\n",
    "\n",
    "\n",
    "# for filename in sorted(os.listdir(d2)):\n",
    "#     fn = os.path.join(d2, filename)\n",
    "#     # checking if it is a file\n",
    "#     if os.path.isfile(fn):\n",
    "#         with open(fn) as f:\n",
    "#             lines = f.readlines()\n",
    "#             aline = []\n",
    "#             for x in lines:\n",
    "#                 aline.append([int(y) for y in x.rstrip('\\n').split(\" \")[-2:]])\n",
    "#             data_out.append(aline)\n",
    "#         f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa3994fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_in = data_in[:len(data_out)]\n",
    "# # print(tf.constant(data_in).shape)\n",
    "# # print(tf.constant(data_out).shape)\n",
    "\n",
    "# data = []\n",
    "# sequence_len = 16\n",
    "# ds = tf.data.Dataset.from_tensor_slices((tf.constant(data_in), tf.constant(data_out)))\n",
    "# ds = ds.batch(sequence_len)\n",
    "# ds = ds.shuffle(ds.cardinality().numpy())\n",
    "# for inn, outt in ds:\n",
    "#     print(inn.shape, outt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aad57b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GU(Model):\n",
    "\n",
    "    def __init__(self, in_dim, num_channels=2, emb_dim=512, gate_filters=32, num_resolutions=2, attn_res_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.num_resolutions = num_resolutions\n",
    "        self.attn_res_idx = attn_res_idx\n",
    "        self.emb_dim = emb_dim\n",
    "        self.gate_filters = gate_filters\n",
    "#         self.model = self.build_graph()\n",
    "        \n",
    "#     def timestep_embedding(self, ts, embedding_dim):\n",
    "#         assert len(ts.shape)==1\n",
    "        \n",
    "#         half_dim = embedding_dim // 2\n",
    "#         emb = -(tf.math.log(10000.0)/(half_dim-1))\n",
    "        \n",
    "#         idxs = tf.range(half_dim, dtype=tf.float32)\n",
    "#         emb = tf.math.exp(idxs*emb)\n",
    "        \n",
    "#         ts = tf.cast(ts, dtype=tf.float32)\n",
    "        \n",
    "#         emb = ts[:,None]* emb[None,:]\n",
    "        \n",
    "#         pe = tf.concat([tf.sin(emb), tf.cos(emb)], axis=1)\n",
    "        \n",
    "#         return pe\n",
    "    \n",
    "    def nonlinearity(self, x):\n",
    "        return tf.keras.activations.swish(x)\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        return tfa.layers.InstanceNormalization(axis=-1)(x)\n",
    "        \n",
    "    def ResBlK(self, x, temb=None):\n",
    "        h = self.nonlinearity(self.normalize(x))\n",
    "        h = layers.Conv2D(x.shape[-1], (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(h)\n",
    "        if not temb == None:\n",
    "            temb = self.nonlinearity(temb)\n",
    "            h +=  tf.reshape(layers.Dense(h.shape[-1])(temb), (-1, 1, 1,h.shape[-1]))\n",
    "\n",
    "        h = self.nonlinearity(self.normalize(h))\n",
    "        h = layers.Conv2D(x.shape[-1], (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(h)\n",
    "        \n",
    "        return x+h\n",
    "        \n",
    "    def downsample(self, x, filters, factor=2):\n",
    "        return layers.Conv2D(filters, (3, 3),\n",
    "                                        strides=(factor, factor), padding='same')(x)\n",
    "\n",
    "    def upsample(self, x, filters, factor=2):\n",
    "        return layers.Conv2DTranspose(filters, (3, 3),\n",
    "                                       strides=(factor, factor), padding='same')(x)\n",
    "        \n",
    "    def NonlocalGaussian(self, x):\n",
    "        h = self.normalize(x)\n",
    "        theta = layers.Conv2D(x.shape[-1]/2.0, (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(h)\n",
    "        phi = layers.Conv2D(x.shape[-1]/2.0, (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(h)\n",
    "        g = layers.Conv2D(x.shape[-1]/2.0, (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(h)\n",
    "        phi = tf.transpose(phi, (0, 3, 2, 1))\n",
    "\n",
    "        theta = tf.reshape(theta, shape=(-1, theta.shape[1]*theta.shape[2], theta.shape[3]))\n",
    "\n",
    "        phi = tf.reshape(phi, shape=(-1, phi.shape[1], phi.shape[2]*phi.shape[3]))\n",
    "        g = tf.reshape(g, shape=(-1, g.shape[1]*g.shape[2], g.shape[3]))\n",
    "        \n",
    "        f = tf.matmul(theta, phi)\n",
    "        \n",
    "        f = layers.Softmax()(f)\n",
    "        \n",
    "        y = tf.matmul(f, g)\n",
    "        \n",
    "        y = tf.reshape(y, (-1, x.shape[1], x.shape[2], y.shape[-1]))\n",
    "        \n",
    "        z = tf.math.add(x, layers.Conv2D(x.shape[-1], (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(y))\n",
    "        return z\n",
    "    \n",
    "    def design(self, img_input):\n",
    "        \n",
    "        hs = [layers.Conv2D(self.gate_filters, (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(img_input)]\n",
    "        \n",
    "        for i in range(self.num_resolutions):\n",
    "            h = self.ResBlK(hs[-1])\n",
    "            if i == self.attn_res_idx:\n",
    "                h = self.NonlocalGaussian(h)\n",
    "                \n",
    "            if i != self.num_resolutions - 1:\n",
    "                h = self.downsample(h, h.shape[-1]*2)\n",
    "                \n",
    "            hs.append(h)\n",
    "\n",
    "        h = hs[-1]\n",
    "        h = self.ResBlK(h)\n",
    "        h = self.NonlocalGaussian(h)\n",
    "        h = self.ResBlK(h)\n",
    "                \n",
    "        for i in reversed(range(self.num_resolutions)):\n",
    "            if i == self.attn_res_idx:\n",
    "                h = self.NonlocalGaussian(h)   \n",
    "            h = self.ResBlK(h)\n",
    "            h = h+hs[i]\n",
    "                \n",
    "            if i != 0:\n",
    "                h = self.upsample(h, h.shape[-1]//2)\n",
    "                \n",
    "        h = self.nonlinearity(self.normalize(h))\n",
    "        \n",
    "        h = layers.Conv2D(self.num_channels, (1, 1),\n",
    "                                        strides=(1, 1), padding='same')(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "#     def call(self, x):\n",
    "#         return self.model([x])\n",
    "    \n",
    "    def build_model(self):\n",
    "        x = layers.Input(shape=(self.in_dim[0],\n",
    "                                        self.in_dim[1], self.num_channels))\n",
    "        \n",
    "        return Model(inputs=[x], outputs=self.design(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e72f5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet1=GU((16, 68))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd71dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = unet1.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3196126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet1(tf.zeros(shape=(16,68,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8f35410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model1,to_file=\"./imgs/model.png\", show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88de40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-16\n",
    "\n",
    "class model:\n",
    "    def __init__(self, dataset_path, data_shape, num_channels, dum=5, train_split=0.8,\n",
    "                 batch_size=64, lr=3e-4):\n",
    "        self.s = data_shape\n",
    "        self.nc = num_channels\n",
    "        self.bs = batch_size\n",
    "        self.dum = dum\n",
    "        \n",
    "        if not dataset_path==None:\n",
    "            cl_path = dataset_path + \"/client\"\n",
    "            co_path = dataset_path + \"/counselor\"\n",
    "\n",
    "            self.ds = self.make_dataset(cl_path, co_path)\n",
    "            \n",
    "            ds_size = self.ds.cardinality().numpy()\n",
    "            train_size = int(train_split * ds_size)\n",
    "            test_size = int((1.0-train_split) * ds_size)\n",
    "\n",
    "            self.train_ds = self.ds.take(train_size)    \n",
    "            self.test_ds = self.ds.skip(train_size)\n",
    "        else:\n",
    "            print(\"WARNING: Dataset not loaded, Model in Generator mode\")\n",
    "        \n",
    "        self.g = U_Net(self.s, self.nc)\n",
    "        self.g_opt = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    def make_dataset(self, d1, d2):\n",
    "        data_in = []\n",
    "        data_out = []\n",
    "        for filename in sorted(os.listdir(d1)):\n",
    "            fn = os.path.join(d1, filename)\n",
    "            if os.path.isfile(fn):\n",
    "                with open(fn) as f:\n",
    "                    lines = f.readlines()\n",
    "                    aline = []\n",
    "                    for x in lines:\n",
    "                        aline.append([int(y) for y in x.rstrip('\\n').split(\" \")[-2:]])\n",
    "                    data_in.append(aline)\n",
    "                f.close()\n",
    "\n",
    "\n",
    "        for filename in sorted(os.listdir(d2)):\n",
    "            fn = os.path.join(d2, filename)\n",
    "            # checking if it is a file\n",
    "            if os.path.isfile(fn):\n",
    "                with open(fn) as f:\n",
    "                    lines = f.readlines()\n",
    "                    aline = []\n",
    "                    for x in lines:\n",
    "                        aline.append([int(y) for y in x.rstrip('\\n').split(\" \")[-2:]])\n",
    "                    data_out.append(aline)\n",
    "                f.close()\n",
    "\n",
    "        data_in = data_in[:len(data_out)]\n",
    "        data = []\n",
    "        sequence_len = self.s[0]\n",
    "        ds = tf.data.Dataset.from_tensor_slices((tf.constant(data_in), tf.constant(data_out)))\n",
    "        ds = ds.batch(sequence_len)\n",
    "        ds = ds.shuffle(ds.cardinality().numpy())\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    def loss_func(self, pred_co, gt_co):\n",
    "        return tf.reduce_mean(tf.math.sqrt(tf.reduce_sum(tf.math.add(pred_co, -tf.cast(gt_co, tf.float32))**2, axis = [1,2,3])+EPSILON))\n",
    "        \n",
    "    @tf.function\n",
    "    def update(self, cl, co):\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            pred_co = self.g(cl)\n",
    "            g_loss = self.loss_func(pred_co, co)\n",
    "            \n",
    "        grad_g = g_tape.gradient(g_loss, self.g.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(grad_g, self.g.trainable_variables))\n",
    "        \n",
    "        return g_loss\n",
    "        \n",
    "    def train(self, epochs=50):\n",
    "        for epo in range(epochs):\n",
    "            g_losses = []\n",
    "            \n",
    "            batch_inn = []\n",
    "            batch_outt = []\n",
    "\n",
    "            for inn, outt in self.train_ds:\n",
    "                if inn.shape == (8,68,2) and outt.shape == (8,68,2):\n",
    "                    if (len(batch_inn))%self.bs ==0 and not len(batch_inn) == 0:\n",
    "                        tf_batch_inn = tf.stack(batch_inn)\n",
    "                        tf_batch_outt = tf.stack(batch_outt)\n",
    "                        g_losses.append(self.update(tf_batch_inn, tf_batch_outt))\n",
    "                        batch_inn = []\n",
    "                        batch_outt = []\n",
    "                    else:\n",
    "                        batch_inn.append(inn)\n",
    "                        batch_outt.append(outt)\n",
    "\n",
    "\n",
    "            print(\"Epoch {:04d}\".format(epo), \"Generator Avg. Loss: \", np.mean(g_losses), flush=True)\n",
    "    def test(self):\n",
    "        errors = []\n",
    "        batch_inn = []\n",
    "        batch_outt = []\n",
    "        for inn, outt in self.test_ds:\n",
    "            if inn.shape == (8,68,2) and outt.shape == (8,68,2):\n",
    "                if (len(batch_inn))%self.bs ==0 and not len(batch_inn) == 0:\n",
    "                    tf_batch_inn = tf.stack(batch_inn)\n",
    "                    tf_batch_outt = tf.stack(batch_outt)\n",
    "                    pred_co = self.g(tf_batch_inn)\n",
    "                    errors.append(self.loss_func(pred_co, tf_batch_outt))\n",
    "                    batch_inn = []\n",
    "                    batch_outt = []\n",
    "                else:\n",
    "                    batch_inn.append(inn)\n",
    "                    batch_outt.append(outt)\n",
    "        print(\"Test Set Avg. Loss: \", np.mean(errors), flush=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7f2b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"/home/data_backup/data_bu/eva_dataset/materialForEEVA/images/session2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afcc2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = model(ds_path, (8,68), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d575cfc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# m1.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea8df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a68e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_size=32\n",
    "# batch_inn = []\n",
    "# batch_outt = []\n",
    "\n",
    "\n",
    "# for inn, outt in m1.ds:\n",
    "#     if inn.shape == (8,68,2) and outt.shape == (8,68,2):\n",
    "#         if (len(batch_inn))%batch_size ==0:\n",
    "#             tf_batch_inn = tf.stack(batch_inn)\n",
    "#             tf_batch_outt = tf.stack(batch_outt)\n",
    "#             if not tf_batch_inn.shape[0] == 0:\n",
    "#                 pred_out = m1.g(tf_batch_inn)\n",
    "#                 print(pred_out.shape)\n",
    "#             batch_inn = []\n",
    "#             batch_outt = []\n",
    "#             batch_inn.append(inn)\n",
    "#             batch_outt.append(outt)\n",
    "#         else:\n",
    "#             batch_inn.append(inn)\n",
    "#             batch_outt.append(outt)\n",
    "#         print(batch_outt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0baaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
