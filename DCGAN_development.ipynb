{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib tensorflow tensorflow_addons tensorflow_datasets imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "# #   \"/home/tony/TO_BE_REMOVED/celeba_data/imgs/\",\n",
    "#   \"/Users/anthonylaw/Desktop/Endless/GAN-devel/mnist_ds/mnist_jpg/training\",\n",
    "#   seed=123,\n",
    "#   image_size=(32, 32),\n",
    "#   batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i_b, l_b in train_ds:\n",
    "#     print(i_b.shape)\n",
    "#     print(tf.image.rgb_to_grayscale(i_b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images should be normalized to [-1,1] ***(Done in Arch)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the network size for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "\n",
    "    def __init__(self, noise_dim, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.noise_dim = noise_dim\n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        self.lr_d = layers.ReLU()\n",
    "        self.lr_c1 = layers.ReLU()\n",
    "        self.lr_c2 = layers.ReLU()\n",
    "        self.lr_c3 = layers.ReLU()\n",
    "        \n",
    "        self.init_dense = layers.Dense(image_shape[0]/8.0*image_shape[1]/8.0*64,\n",
    "                               use_bias=False, input_shape=(None, self.noise_dim))\n",
    "        \n",
    "        self.init_reshape = layers.Reshape((int(image_shape[0]/8.0), int(image_shape[1]/8.0), 64))\n",
    "        \n",
    "        self.conv2dT1 = layers.Conv2DTranspose(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        self.conv2dT2 = layers.Conv2DTranspose(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2dT3 = layers.Conv2DTranspose(16, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2dTactv = layers.Conv2DTranspose(self.num_channel, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same', activation='tanh')\n",
    "\n",
    "    def call(self, noise_vec):\n",
    "\n",
    "        init_vec = self.lr_d(self.init_dense(noise_vec))\n",
    "                \n",
    "        reshaped = self.init_reshape(init_vec)\n",
    "        \n",
    "        convt1 = self.lr_c1(self.conv2dT1(reshaped))\n",
    "        \n",
    "        convt2 = self.lr_c2(self.conv2dT2(convt1))\n",
    "                         \n",
    "        convt3 = self.lr_c3(self.conv2dT3(convt2))\n",
    "            \n",
    "        out = self.conv2dTactv(convt3)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def build_graph(self):\n",
    "        x = layers.Input(shape=(self.noise_dim,))\n",
    "        \n",
    "        return Model(inputs=x, outputs=self.call(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1 = Generator(10, (32, 32), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_input = tf.random.normal((5, 10))\n",
    "# print(noise_input.shape)\n",
    "# pics1 = g1(noise_input)\n",
    "# print(pics1.shape)\n",
    "# # plt.imshow(pics1[-1, :, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(g1.build_graph(), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1.build_graph().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "\n",
    "    def __init__(self, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        self.lr_c1 = layers.LeakyReLU()\n",
    "        self.lr_c2 = layers.LeakyReLU()\n",
    "        self.lr_c3 = layers.LeakyReLU()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.conv2d1 = layers.Conv2D(16, (self.kernel_size, self.kernel_size),\n",
    "                                        strides=(2, 2), padding='same',\n",
    "                                        input_shape=(None, self.image_shape[0],\n",
    "                                        self.image_shape[1], self.num_channel))\n",
    "        self.conv2d2 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2d3 = layers.Conv2D(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.dense_actv = layers.Dense(64,\n",
    "                                      )\n",
    "#                                        activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, img_input):\n",
    "        \n",
    "        conv1 = self.lr_c1(self.conv2d1(img_input))\n",
    "\n",
    "        conv2 = self.lr_c2(self.conv2d2(conv1))\n",
    "                         \n",
    "        conv3 = self.lr_c3(self.conv2d3(conv2))\n",
    "\n",
    "        flat = self.flatten(conv3)\n",
    "        \n",
    "        out = self.dense_actv(flat)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = layers.Input(shape=(self.image_shape[0],\n",
    "                                        self.image_shape[1], self.num_channel))\n",
    "        \n",
    "        return Model(inputs=x, outputs=self.call(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1 = Discriminator((32,32), 3)\n",
    "# g2 = Generator(10, (32,32), 3)\n",
    "# d1.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_input = tf.random.normal((5, 10))\n",
    "# pics2 = g1(noise_input)\n",
    "# # plt.imshow(pics2[-1, :, :, 0], cmap='gray')\n",
    "# print(pics2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# deci = d1(pics2)\n",
    "# deci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1.build_graph().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CramerDCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-16\n",
    "\n",
    "class DCGAN:\n",
    "    \n",
    "    def __init__(self, dataset_path, image_shape, num_channel, noise_latent_dim, disc_update_multi=5, \n",
    "                 batch_size=64, lr=3e-4, gp_lam = 10.0, div_lam = 0.1):\n",
    "        assert len(image_shape) == 2\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.noise_latent_dim = noise_latent_dim\n",
    "        self.batch_size, self.gp_lam = batch_size, gp_lam\n",
    "        self.disc_update_multi = disc_update_multi\n",
    "        self.num_img_prog_monit = 16\n",
    "        self.div_lam = div_lam\n",
    "        \n",
    "        if not dataset_path==None:\n",
    "            self.dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "                                  dataset_path,\n",
    "                                  seed=123,\n",
    "                                  image_size=self.image_shape,\n",
    "                                  batch_size=self.batch_size)\n",
    "            \n",
    "        else:\n",
    "            print(\"WARNING: Dataset not loaded, Model in Generator mode\")\n",
    "        # NOTE: Dataset must be processed differently for different source and applications\n",
    "        \n",
    "        self.g = Generator(self.noise_latent_dim, self.image_shape, self.num_channel)\n",
    "        self.d = Discriminator(self.image_shape, self.num_channel)\n",
    "        \n",
    "        self.g_opt = tf.keras.optimizers.Adam(lr)\n",
    "        self.d_opt = tf.keras.optimizers.Adam(lr)\n",
    "        \n",
    "        self.g_seed = tf.random.normal((self.num_img_prog_monit, self.noise_latent_dim))\n",
    "\n",
    "    def cramer_loss(self, d_x_data, d_g_z_1, d_g_z_2, x_it):\n",
    "        \n",
    "        crit_r = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_x_data, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_x_data**2, axis = 1)+EPSILON))\n",
    "        crit_g_1 = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_g_z_1, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_g_z_1**2, axis = 1)+EPSILON))\n",
    "\n",
    "        L_srg = tf.math.add(crit_r, -crit_g_1)\n",
    "        \n",
    "        with tf.GradientTape() as t_gp:\n",
    "            t_gp.watch(x_it)\n",
    "            d_it = self.d(x_it)\n",
    "            crit_it = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_it, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(tf.math.add(d_it, -d_x_data)**2, axis = 1)+EPSILON))\n",
    "            \n",
    "        gp_grad = t_gp.gradient(crit_it, x_it)\n",
    "        l2n_gp = tf.math.sqrt(tf.reduce_sum(gp_grad**2, axis = [1,2,3])+EPSILON)\n",
    "        L_gp = self.gp_lam*((l2n_gp-1.0)**2)\n",
    "\n",
    "\n",
    "        # g_loss\n",
    "        L_g = tf.reduce_mean(L_srg)\n",
    "        \n",
    "        # d_loss\n",
    "        L_d = tf.reduce_mean(-L_srg + L_gp)\n",
    "\n",
    "        return L_g, L_d\n",
    "    \n",
    "    def div_loss(self, g_z1, g_z2):\n",
    "\n",
    "        L_div = tf.math.sqrt(tf.reduce_sum(tf.math.add(g_z1, -g_z2)**2, axis = [1,2,3])+EPSILON)\n",
    "\n",
    "        return tf.reduce_mean(L_div)\n",
    "    \n",
    "    @tf.function\n",
    "    def update(self, imgs, update_gen=True):\n",
    "        noise_input1 = tf.random.normal((imgs.shape[0], self.noise_latent_dim))\n",
    "        noise_input2 = tf.random.normal((imgs.shape[0], self.noise_latent_dim))\n",
    "        \n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            g_z_1 = self.g(noise_input1)\n",
    "            g_z_2 = self.g(noise_input2)\n",
    "            \n",
    "            d_x_data = self.d(imgs)\n",
    "            d_g_z_1 = self.d(g_z_1)\n",
    "            d_g_z_2 = self.d(g_z_2)\n",
    "            \n",
    "            epsi = tf.random.uniform([imgs.shape[0], 1, 1, 1], 0.0, 1.0)\n",
    "            x_it = tf.math.add(epsi*imgs, (1.0-epsi)*g_z_1)\n",
    "            g_loss, d_loss = self.cramer_loss(d_x_data, d_g_z_1, d_g_z_2, x_it)\n",
    "            div_loss = self.div_loss(g_z_1, g_z_2)\n",
    "            g_loss = g_loss-self.div_lam*div_loss\n",
    "\n",
    "        if update_gen:\n",
    "            grad_g = g_tape.gradient(g_loss, self.g.trainable_variables)\n",
    "            grad_d = d_tape.gradient(d_loss, self.d.trainable_variables)\n",
    "\n",
    "            self.g_opt.apply_gradients(zip(grad_g, self.g.trainable_variables))\n",
    "            self.d_opt.apply_gradients(zip(grad_d, self.d.trainable_variables))\n",
    "        else:\n",
    "            grad_d = d_tape.gradient(d_loss, self.d.trainable_variables)\n",
    "            self.d_opt.apply_gradients(zip(grad_d, self.d.trainable_variables))\n",
    "            \n",
    "        return g_loss, d_loss\n",
    "        \n",
    "    def train(self, epochs=250, train_moni_path=None, g_path=None, d_path=None):\n",
    "        num_training = 0\n",
    "        for epo in range(epochs):\n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            for img_b, l_b in self.dataset:\n",
    "                if self.num_channel == 1 and img_b.shape[-1] == 3:\n",
    "                    img_b = tf.image.rgb_to_grayscale(img_b)\n",
    "                img_b = (img_b-127.5)/127.5    \n",
    "                if num_training%self.disc_update_multi == 0:\n",
    "                    g_l, d_l = self.update(img_b, True)\n",
    "                    g_losses.append(g_l.numpy())\n",
    "                    d_losses.append(d_l.numpy())\n",
    "                    \n",
    "                else:\n",
    "                    g_l, d_l = self.update(img_b, False)\n",
    "                    d_losses.append(d_l.numpy())\n",
    "                    \n",
    "                num_training = (num_training+1)%self.disc_update_multi\n",
    "                \n",
    "            print(\"Epoch {:04d}\".format(epo), \"Generator Avg. Loss: \", np.mean(g_losses), \n",
    "                  \", Discriminator Avg. Loss: \",  np.mean(d_losses), flush=True)\n",
    "            \n",
    "            if not g_path==None and not d_path==None:\n",
    "                self.save_weights(g_path, d_path)\n",
    "                \n",
    "            if not train_moni_path == None:\n",
    "                self.monitor_progress(epo, train_moni_path)\n",
    "            \n",
    "    def monitor_progress(self, epo, path):\n",
    "        pics = self.g(self.g_seed)\n",
    "        \n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        for i in range(pics.shape[0]):\n",
    "            plt.subplot(4,4,i+1)\n",
    "            if self.num_channel == 1:\n",
    "                plt.imshow(pics[i,:,:,0], cmap='gray')\n",
    "            else:   \n",
    "                plt.imshow(tf.cast(tf.math.round(pics[i,:,:,:]*127.5+127.5), tf.int32))\n",
    "            plt.axis('off')\n",
    "            \n",
    "        plt.savefig(path+'/image_{:04d}.png'.format(epo))\n",
    "#         plt.savefig('/home/tony/TO_BE_REMOVED/imgs/image_{:04d}.png'.format(epo))\n",
    "        # NEEDS to be changed for machines\n",
    "        \n",
    "        plt.close('all')\n",
    "        \n",
    "    def save_weights(self, g_path, d_path):\n",
    "        self.g.save_weights(g_path)\n",
    "        print(\"Saved generator weights\", flush=True)\n",
    "        self.d.save_weights(d_path)\n",
    "        print(\"Saved discriminator weights\", flush=True)\n",
    "    def load_weights(self, g_path, d_path):\n",
    "        try:\n",
    "            self.g.load_weights(g_path).expect_partial()\n",
    "            print(\"Loaded generator weights\", flush=True)\n",
    "            self.d.load_weights(d_path).expect_partial()\n",
    "            print(\"Loaded discriminator weights\", flush=True)\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "    \n",
    "    def generate_samples(self, num_sam, path):\n",
    "        sam_seed = tf.random.normal((num_sam, self.noise_latent_dim))\n",
    "        sam_pics = self.g(sam_seed)\n",
    "        \n",
    "        dpi = matplotlib.rcParams['figure.dpi']\n",
    "        \n",
    "        for i in range(sam_pics.shape[0]):\n",
    "            \n",
    "            figsize = self.image_shape[1] / float(dpi), self.image_shape[0] / float(dpi)\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            ax = fig.add_axes([0, 0, 1, 1])\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if self.num_channel == 1:\n",
    "                ax.imshow(sam_pics[i,:,:,0], cmap='gray', interpolation='nearest')\n",
    "            else:   \n",
    "                ax.imshow(tf.cast(tf.math.round(sam_pics[i,:,:,:]*127.5+127.5), tf.int32), interpolation='nearest')\n",
    "\n",
    "            fig.savefig(path+'/image_{:04d}.png'.format(i), dpi=dpi, transparent=True)\n",
    "            plt.close('all')\n",
    "            \n",
    "    def model_params(self):\n",
    "        self.g.build_graph().summary()\n",
    "        self.d.build_graph().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Img save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpi = matplotlib.rcParams['figure.dpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_data = plt.imread('/home/data_backup/data_bu/mnist_jpg/training/9/.vou/GAN-devel/model.png')\n",
    "# height, width, _ = im_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize = width / float(dpi), height / float(dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=figsize)\n",
    "# ax = fig.add_axes([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax.imshow(im_data, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('test.png', dpi=dpi, transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_path = \"/home/data_backup/Downloads/archive/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/train400Xpng\"\n",
    "ds_path = \"/home/szorek/mnist/mnist_jpg\"\n",
    "moni_path = \"/home/szorek/toxoplasmosis-research/samples/gan_moni\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 19:35:39.727655: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 19:35:40.110526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21388 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "dcgan1 = DCGAN(ds_path, (28, 28), 1, 32, disc_update_multi=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan1.model_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 19:36:04.441364: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-03-22 19:36:04.893817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Generator Avg. Loss:  7.784465 , Discriminator Avg. Loss:  -7.2073035\n",
      "Epoch 0001 Generator Avg. Loss:  4.7792363 , Discriminator Avg. Loss:  -5.123457\n",
      "Epoch 0002 Generator Avg. Loss:  3.6593077 , Discriminator Avg. Loss:  -4.1692286\n",
      "Epoch 0003 Generator Avg. Loss:  3.1466382 , Discriminator Avg. Loss:  -3.7634003\n",
      "Epoch 0004 Generator Avg. Loss:  3.1108131 , Discriminator Avg. Loss:  -3.3040166\n",
      "Epoch 0005 Generator Avg. Loss:  1.4628533 , Discriminator Avg. Loss:  -2.3284109\n",
      "Epoch 0006 Generator Avg. Loss:  1.0110052 , Discriminator Avg. Loss:  -2.0127344\n",
      "Epoch 0007 Generator Avg. Loss:  0.90865517 , Discriminator Avg. Loss:  -1.0592635\n",
      "Epoch 0008 Generator Avg. Loss:  -0.54232895 , Discriminator Avg. Loss:  -0.5342614\n",
      "Epoch 0009 Generator Avg. Loss:  -0.24669757 , Discriminator Avg. Loss:  -0.87874955\n",
      "Epoch 0010 Generator Avg. Loss:  0.44620368 , Discriminator Avg. Loss:  -1.511617\n",
      "Epoch 0011 Generator Avg. Loss:  0.6013062 , Discriminator Avg. Loss:  -1.6124732\n",
      "Epoch 0012 Generator Avg. Loss:  -0.66352993 , Discriminator Avg. Loss:  -0.071788\n",
      "Epoch 0013 Generator Avg. Loss:  -0.288254 , Discriminator Avg. Loss:  -0.9093993\n",
      "Epoch 0014 Generator Avg. Loss:  -0.30068362 , Discriminator Avg. Loss:  -0.91954046\n",
      "Epoch 0015 Generator Avg. Loss:  0.3777149 , Discriminator Avg. Loss:  -1.4179422\n",
      "Epoch 0016 Generator Avg. Loss:  -0.020087233 , Discriminator Avg. Loss:  -0.6158802\n",
      "Epoch 0017 Generator Avg. Loss:  -0.38578266 , Discriminator Avg. Loss:  -0.7322139\n",
      "Epoch 0018 Generator Avg. Loss:  -0.50020415 , Discriminator Avg. Loss:  -0.7816947\n",
      "Epoch 0019 Generator Avg. Loss:  -0.6449159 , Discriminator Avg. Loss:  0.117479466\n"
     ]
    }
   ],
   "source": [
    "dcgan1.train(20,moni_path)\n",
    "# dcgan1.save_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan1.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan1.generate_samples(10, './samples/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan2 = DCGAN(ds_path, (32, 32), 1, 25, disc_update_multi=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan2.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dcgan2.train(20, './imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan2.save_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan1.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan3 = DCGAN(None, (32, 32), 1, 25, disc_update_multi=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan3.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcgan3.generate_samples(10, './samples/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Act Resblk Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "\n",
    "    def __init__(self, noise_dim, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%4 == 0\n",
    "        assert image_shape[1]%4 == 0\n",
    "        \n",
    "        self.noise_dim = noise_dim\n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = 3\n",
    "        \n",
    "        # add layer(multiple for plot clarification)\n",
    "        self.add1 = layers.Add()\n",
    "        self.add2 = layers.Add()\n",
    "        \n",
    "        # init layers\n",
    "        self.init_dense = layers.Dense(image_shape[0]/4.0*image_shape[1]/4.0,\n",
    "                               use_bias=False, input_shape=(None, self.noise_dim), activation='relu')\n",
    "        \n",
    "        self.init_reshape = layers.Reshape((int(image_shape[0]/4.0), int(image_shape[1]/4.0),1))\n",
    "        \n",
    "        # in conv 1x1\n",
    "        self.in_conv11 = layers.Conv2D(64, (1, 1),\n",
    "                                strides=(1, 1), padding='same')\n",
    "        \n",
    "        # resblk 1\n",
    "        self.rb1_in1 = tfa.layers.InstanceNormalization(axis=-1)\n",
    "        self.rb1_rl1 = layers.ReLU()\n",
    "        self.rb1_conv2d1 = layers.Conv2D(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        self.rb1_in2 = tfa.layers.InstanceNormalization(axis=-1)\n",
    "        self.rb1_rl2 = layers.ReLU()\n",
    "        self.rb1_conv2d2 = layers.Conv2D(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        # upsample 1\n",
    "        self.us1 = layers.Conv2DTranspose(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        \n",
    "        # resblk2\n",
    "        self.rb2_in1 = tfa.layers.InstanceNormalization(axis=-1)\n",
    "        self.rb2_rl1 = layers.ReLU()\n",
    "        self.rb2_conv2d1 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        self.rb2_in2 = tfa.layers.InstanceNormalization(axis=-1)\n",
    "        self.rb2_rl2 = layers.ReLU()\n",
    "        self.rb2_conv2d2 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        # upsample 2\n",
    "        self.us2 = layers.Conv2DTranspose(16, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        \n",
    "        # out conv 1x1\n",
    "        self.out_conv11 = layers.Conv2D(self.num_channel, (1, 1),\n",
    "                                        strides=(1, 1), padding='same', activation='tanh')\n",
    "\n",
    "    def call(self, noise_vec):\n",
    "        \n",
    "        # in\n",
    "        x = self.init_reshape(self.init_dense(noise_vec))\n",
    "#         x = self.init_reshape(noise_vec)\n",
    "        \n",
    "        # conv 1x1\n",
    "        x = self.in_conv11(x)\n",
    "        \n",
    "        # resblk 1\n",
    "        x_res = self.rb1_conv2d1(self.rb1_rl1(self.rb1_in1(x)))\n",
    "        x_res = self.rb1_conv2d2(self.rb1_rl2(self.rb1_in2(x_res)))\n",
    "        x = self.add1([x, x_res])\n",
    "        \n",
    "        # upsample 1\n",
    "        x = self.us1(x)\n",
    "        \n",
    "        # resblk 2\n",
    "        x_res = self.rb2_conv2d1(self.rb2_rl1(self.rb2_in1(x)))\n",
    "        x_res = self.rb2_conv2d2(self.rb2_rl2(self.rb2_in2(x_res)))\n",
    "        x = self.add2([x, x_res])\n",
    "        \n",
    "        # upsample 2\n",
    "        x = self.us2(x)\n",
    "        \n",
    "        # conv 1x1\n",
    "        x = self.out_conv11(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = layers.Input(shape=(self.noise_dim,))\n",
    "        \n",
    "        return Model(inputs=x, outputs=self.call(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1 = Generator(10, (32, 32), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_input = tf.random.normal((5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(g1.build_graph(), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1.build_graph().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Act Resblk Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "\n",
    "    def __init__(self, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = 3\n",
    "        \n",
    "        # add layer(multiple for plot clarification)\n",
    "        self.add1 = layers.Add()\n",
    "        self.add2 = layers.Add()\n",
    "        \n",
    "        \n",
    "        # in conv 1x1\n",
    "        self.in_conv11 = layers.Conv2D(16, (1, 1),\n",
    "                                        strides=(1, 1), padding='same',\n",
    "                                        input_shape=(None, self.image_shape[0],\n",
    "                                        self.image_shape[1], self.num_channel))\n",
    "        \n",
    "        # resblk 1 \n",
    "        self.rb1_rl1 = layers.LeakyReLU()\n",
    "        self.rb1_conv2d1 = layers.Conv2D(16, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        self.rb1_rl2 = layers.LeakyReLU()\n",
    "        self.rb1_conv2d2 = layers.Conv2D(16, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        # downsample 1\n",
    "        self.ds1 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        \n",
    "        # resblk 2\n",
    "        self.rb2_rl1 = layers.LeakyReLU()\n",
    "        self.rb2_conv2d1 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        self.rb2_rl2 = layers.LeakyReLU()\n",
    "        self.rb2_conv2d2 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        \n",
    "        # downsample 2\n",
    "        self.ds2 = layers.Conv2D(8, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        \n",
    "        # reshape for linear act\n",
    "        self.rs_out = layers.Reshape((int(self.image_shape[0]/4.0)*int(self.image_shape[1]/4.0)*8,))\n",
    "        \n",
    "        # out\n",
    "        self.dense_actv = layers.Dense(8)\n",
    "\n",
    "        \n",
    "        \n",
    "    def call(self, img_input):\n",
    "        # in\n",
    "        x = self.in_conv11(img_input)\n",
    "        \n",
    "        # resblk 1\n",
    "        x_res = self.rb1_conv2d1(self.rb1_rl1(x))\n",
    "        x_res = self.rb1_conv2d2(self.rb1_rl2(x_res))\n",
    "        x = self.add1([x, x_res])\n",
    "        \n",
    "        # resblk 1 downsample\n",
    "        x = self.ds1(x)\n",
    "        \n",
    "        # resblk 2\n",
    "        x_res = self.rb2_conv2d1(self.rb2_rl1(x))\n",
    "        x_res = self.rb2_conv2d2(self.rb2_rl2(x_res))\n",
    "        x = self.add2([x, x_res])\n",
    "        \n",
    "        # resblk 2 downsample\n",
    "        x = self.ds2(x)\n",
    "        \n",
    "        # reshape\n",
    "        x = self.rs_out(x)\n",
    "        \n",
    "        # out\n",
    "        x = self.dense_actv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def build_graph(self):\n",
    "        x = layers.Input(shape=(self.image_shape[0],\n",
    "                                        self.image_shape[1], self.num_channel))\n",
    "        \n",
    "        return Model(inputs=x, outputs=self.call(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1 = Discriminator((160, 120), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(d1.build_graph(), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
