{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: tensorflow in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow_addons in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: tensorflow_datasets in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (4.6.0)\n",
      "Requirement already satisfied: imageio in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (2.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (1.23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (4.36.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: promise in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (2.28.1)\n",
      "Requirement already satisfied: toml in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (4.64.0)\n",
      "Requirement already satisfied: dill in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (0.3.5.1)\n",
      "Requirement already satisfied: etils[epath] in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (0.7.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (1.9.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.11)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: zipp in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from etils[epath]->tensorflow_datasets) (3.8.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib tensorflow tensorflow_addons tensorflow_datasets imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#   \"/home/tony/TO_BE_REMOVED/celeba_data/imgs/\",\n",
    "  \"/home/tony/TO_BE_REMOVED/mnist_ds/mnist_jpg/training\",\n",
    "  seed=123,\n",
    "  image_size=(32, 32),\n",
    "  batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i_b, l_b in train_ds:\n",
    "#     print(i_b.shape)\n",
    "#     print(tf.image.rgb_to_grayscale(i_b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images should be normalized to [-1,1] ***(Done in Arch)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the network size for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "\n",
    "    def __init__(self, noise_dim, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.noise_dim = noise_dim\n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = int(min(max(min(image_shape[0]/8.0-3.0, image_shape[1]/8.0-3.0), 3.0), 5.0))\n",
    "\n",
    "        self.lr_d = layers.ReLU()\n",
    "        self.lr_c1 = layers.ReLU()\n",
    "        self.lr_c2 = layers.ReLU()\n",
    "        self.lr_c3 = layers.ReLU()\n",
    "        \n",
    "        self.init_dense = layers.Dense(image_shape[0]/8.0*image_shape[1]/8.0*128,\n",
    "                               use_bias=False, input_shape=(self.noise_dim,))\n",
    "        \n",
    "        self.init_reshape = layers.Reshape((int(image_shape[0]/8.0), int(image_shape[1]/8.0), 128))\n",
    "        \n",
    "        self.conv2dT1 = layers.Conv2DTranspose(128, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        self.conv2dT2 = layers.Conv2DTranspose(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2dT3 = layers.Conv2DTranspose(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2dTactv = layers.Conv2DTranspose(self.num_channel, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same', activation='tanh')\n",
    "\n",
    "    def call(self, noise_vec):\n",
    "\n",
    "        init_vec = tf.squeeze(self.lr_d(self.init_dense(noise_vec)))\n",
    "        \n",
    "#         print(init_vec.shape)\n",
    "        \n",
    "        reshaped = self.init_reshape(init_vec)\n",
    "        \n",
    "#         print(reshaped.shape)\n",
    "        \n",
    "        convt1 = self.lr_c1(self.conv2dT1(reshaped))\n",
    "\n",
    "#         print(convt1.shape)\n",
    "        \n",
    "        convt2 = self.lr_c2(self.conv2dT2(convt1))\n",
    "        \n",
    "#         print(convt2.shape)\n",
    "                         \n",
    "        convt3 = self.lr_c3(self.conv2dT3(convt2))\n",
    "        \n",
    "#         print(convt3.shape)\n",
    "            \n",
    "        out = self.conv2dTactv(convt3)\n",
    "        \n",
    "#         print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = Generator(10, (32, 32), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n",
      "(5, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "noise_input = tf.random.normal((5, 10))\n",
    "print(noise_input.shape)\n",
    "pics1 = g1(tf.expand_dims(noise_input, 0))\n",
    "print(pics1.shape)\n",
    "# plt.imshow(pics1[-1, :, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " re_lu_4 (ReLU)              multiple                  0         \n",
      "                                                                 \n",
      " re_lu_5 (ReLU)              multiple                  0         \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              multiple                  0         \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  20480     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  multiple                 147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  multiple                 73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  multiple                 18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  multiple                 867       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,187\n",
      "Trainable params: 261,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "\n",
    "    def __init__(self, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = int(min(max(min(image_shape[0]/8.0-3.0, image_shape[1]/8.0-3.0), 3.0), 5.0))\n",
    "\n",
    "        self.lr_c1 = layers.LeakyReLU()\n",
    "        self.lr_c2 = layers.LeakyReLU()\n",
    "        self.lr_c3 = layers.LeakyReLU()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.conv2d1 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                        strides=(2, 2), padding='same',\n",
    "                                        input_shape=(None, self.image_shape[0],\n",
    "                                        self.image_shape[1], self.num_channel))\n",
    "        self.conv2d2 = layers.Conv2D(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2d3 = layers.Conv2D(128, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.dense_actv = layers.Dense(256,\n",
    "                                      )\n",
    "#                                        activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, img_input):\n",
    "        \n",
    "        conv1 = self.lr_c1(self.conv2d1(img_input))\n",
    "        \n",
    "#         print(conv1.shape)\n",
    "        \n",
    "        conv2 = self.lr_c2(self.conv2d2(conv1))\n",
    "        \n",
    "#         print(conv2.shape)\n",
    "                         \n",
    "        conv3 = self.lr_c3(self.conv2d3(conv2))\n",
    "        \n",
    "#         print(conv3.shape)\n",
    "        \n",
    "        flat = self.flatten(conv3)\n",
    "        \n",
    "#         print(flat.shape)\n",
    "        \n",
    "        out = tf.squeeze(self.dense_actv(flat))\n",
    "        \n",
    "#         print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = Discriminator((32,32), 3)\n",
    "g2 = Generator(10, (32,32), 3)\n",
    "d1.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "noise_input = tf.random.normal((5, 10))\n",
    "pics2 = g1(tf.expand_dims(noise_input, 0))\n",
    "# plt.imshow(pics2[-1, :, :, 0], cmap='gray')\n",
    "print(pics2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 256), dtype=float32, numpy=\n",
       "array([[-1.0242338e-03,  7.4563571e-04,  4.4459841e-05, ...,\n",
       "        -9.2679198e-05,  1.0957530e-03,  3.7814977e-04],\n",
       "       [-9.4595744e-04,  5.6333444e-04, -1.8231114e-04, ...,\n",
       "        -2.5268120e-04,  7.3711824e-04, -1.2429734e-04],\n",
       "       [-1.2060391e-03, -5.1791896e-04, -1.0303807e-03, ...,\n",
       "        -2.6774278e-04,  8.2628761e-04,  5.1804011e-05],\n",
       "       [-9.8223228e-04,  4.9945875e-04, -2.1770452e-04, ...,\n",
       "        -6.2967301e-05,  4.6027196e-04,  4.6945247e-04],\n",
       "       [-1.0700801e-03,  1.0577107e-03,  7.7594472e-05, ...,\n",
       "        -4.8216351e-04,  2.7823698e-04,  1.7998932e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deci = d1(pics2)\n",
    "deci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " leaky_re_lu (LeakyReLU)     multiple                  0         \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   multiple                  0         \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   multiple                  0         \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             multiple                  896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           multiple                  73856     \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  524544    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 617,792\n",
      "Trainable params: 617,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CramerDCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-16\n",
    "\n",
    "class DCGAN:\n",
    "    \n",
    "    def __init__(self, dataset_path, image_shape, num_channel, noise_latent_dim, disc_update_multi=5, \n",
    "                 batch_size=128, lr=3e-4, gp_lam = 10.0):\n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.noise_latent_dim = noise_latent_dim\n",
    "        self.batch_size, self.gp_lam = batch_size, gp_lam\n",
    "        self.disc_update_multi = disc_update_multi\n",
    "        self.num_img_prog_monit = 16\n",
    "        \n",
    "        self.dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "                              dataset_path,\n",
    "                              seed=123,\n",
    "                              image_size=self.image_shape,\n",
    "                              batch_size=self.batch_size)\n",
    "        # NOTE: Dataset must be processed differently for different source and applications\n",
    "        \n",
    "        self.g = Generator(self.noise_latent_dim, self.image_shape, self.num_channel)\n",
    "        self.d = Discriminator(self.image_shape, self.num_channel)\n",
    "        \n",
    "        self.g_opt = tf.keras.optimizers.Adam(lr)\n",
    "        self.d_opt = tf.keras.optimizers.Adam(lr)\n",
    "        \n",
    "        self.g_seed = tf.random.normal((self.num_img_prog_monit, self.noise_latent_dim))\n",
    "\n",
    "    def cramer_loss(self, d_x_data, d_g_z_1, d_g_z_2, x_it, update_gen=True):\n",
    "        \n",
    "        crit_r = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_x_data, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_x_data**2, axis = 1)+EPSILON))\n",
    "        crit_g_1 = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_g_z_1, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_g_z_1**2, axis = 1)+EPSILON))\n",
    "        \n",
    "        L_srg = tf.math.add(crit_r, -crit_g_1)\n",
    "        \n",
    "        with tf.GradientTape() as t_gp:\n",
    "            t_gp.watch(x_it)\n",
    "            d_it = self.d(x_it)\n",
    "            crit_it = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_it, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_it**2, axis = 1)+EPSILON))\n",
    "            \n",
    "        gp_grad = t_gp.gradient(crit_it, x_it)\n",
    "        l2n_gp = tf.math.sqrt(tf.reduce_sum(gp_grad**2, axis = [1,2,3])+EPSILON)\n",
    "        \n",
    "        # d_loss\n",
    "        L_d = tf.reduce_mean(-L_srg + (self.gp_lam*((l2n_gp-1.0)**2)))\n",
    "\n",
    "        if update_gen:\n",
    "            l2nrg1 = tf.math.sqrt(tf.reduce_sum(tf.math.add(d_x_data, -d_g_z_1)**2, axis = 1)+EPSILON)\n",
    "            l2nrg2 = tf.math.sqrt(tf.reduce_sum(tf.math.add(d_x_data, -d_g_z_2)**2, axis = 1)+EPSILON)\n",
    "            l2ng12 = tf.math.sqrt(tf.reduce_sum(tf.math.add(d_g_z_1, -d_g_z_2)**2, axis = 1)+EPSILON)\n",
    "\n",
    "            # g_loss\n",
    "            L_g = tf.reduce_mean(l2nrg1 + l2nrg2 - l2ng12)\n",
    "        else:\n",
    "            L_g = None\n",
    "\n",
    "        return L_g, L_d\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def update(self, imgs, update_gen=True):\n",
    "        noise_input1 = tf.random.normal((imgs.shape[0], self.noise_latent_dim))\n",
    "        noise_input2 = tf.random.normal((imgs.shape[0], self.noise_latent_dim))\n",
    "        \n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            g_z_1 = self.g(noise_input1)\n",
    "            g_z_2 = self.g(noise_input2)\n",
    "            \n",
    "            d_x_data = self.d(imgs)\n",
    "            d_g_z_1 = self.d(g_z_1)\n",
    "            d_g_z_2 = self.d(g_z_2)\n",
    "            \n",
    "            epsi = tf.random.uniform([imgs.shape[0], 1, 1, 1], 0.0, 1.0)\n",
    "            x_it = tf.math.add(epsi*imgs, (1.0-epsi)*g_z_1)\n",
    "            g_loss, d_loss = self.cramer_loss(d_x_data, d_g_z_1, d_g_z_2, x_it, update_gen)\n",
    "\n",
    "        if update_gen:\n",
    "            grad_g = g_tape.gradient(g_loss, self.g.trainable_variables)\n",
    "            grad_d = d_tape.gradient(d_loss, self.d.trainable_variables)\n",
    "\n",
    "            self.g_opt.apply_gradients(zip(grad_g, self.g.trainable_variables))\n",
    "            self.d_opt.apply_gradients(zip(grad_d, self.d.trainable_variables))\n",
    "        else:\n",
    "            grad_d = d_tape.gradient(d_loss, self.d.trainable_variables)\n",
    "            self.d_opt.apply_gradients(zip(grad_d, self.d.trainable_variables))\n",
    "            \n",
    "        return g_loss, d_loss\n",
    "        \n",
    "    def train(self, epochs=250, train_moni_path=None):\n",
    "        num_training = 0\n",
    "        for epo in range(epochs):\n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            for img_b, l_b in self.dataset:\n",
    "                if self.num_channel == 1 and img_b.shape[-1] == 3:\n",
    "                    img_b = tf.image.rgb_to_grayscale(img_b)\n",
    "                norm_img_b = (img_b-127.5)/127.5\n",
    "                if num_training%self.disc_update_multi == 0:\n",
    "                    g_l, d_l = self.update(norm_img_b, True)\n",
    "                    g_losses.append(g_l.numpy())\n",
    "                    d_losses.append(d_l.numpy())\n",
    "                    \n",
    "                else:\n",
    "                    g_l, d_l = self.update(norm_img_b, False)\n",
    "                    d_losses.append(d_l.numpy())\n",
    "                    \n",
    "                num_training = (num_training+1)%self.disc_update_multi\n",
    "                \n",
    "            print(\"Epoch {:04d}\".format(epo), \"Generator Avg. Loss: \", np.mean(g_losses), \n",
    "                  \", Discriminator Avg. Loss: \",  np.mean(d_losses), flush=True)\n",
    "                \n",
    "            if not train_moni_path == None:\n",
    "                self.monitor_progress(epo, train_moni_path)\n",
    "            \n",
    "    def monitor_progress(self, epo, path):\n",
    "        pics = self.g(self.g_seed)\n",
    "        \n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        for i in range(pics.shape[0]):\n",
    "            plt.subplot(4,4,i+1)\n",
    "            if self.num_channel == 1:\n",
    "                plt.imshow(pics[i,:,:,0], cmap='gray')\n",
    "            else:   \n",
    "                plt.imshow(tf.cast(tf.math.round(pics[i,:,:,:]*127.5+127.5), tf.int32))\n",
    "            plt.axis('off')\n",
    "            \n",
    "        plt.savefig(path+'/image_{:04d}.png'.format(epo))\n",
    "#         plt.savefig('/home/tony/TO_BE_REMOVED/imgs/image_{:04d}.png'.format(epo))\n",
    "        # NEEDS to be changed for machines\n",
    "        \n",
    "        plt.close('all')\n",
    "        \n",
    "    def save_weights(self, g_path, d_path):\n",
    "        self.g.save_weights(g_path)\n",
    "        print(\"Saved generator weights\", flush=True)\n",
    "        self.d.save_weights(d_path)\n",
    "        print(\"Saved discriminator weights\", flush=True)\n",
    "    def load_weights(self, g_path, d_path):\n",
    "        try:\n",
    "            self.g.load_weights(g_path)\n",
    "            print(\"Loaded generator weights\", flush=True)\n",
    "            self.d.load_weights(d_path)\n",
    "            print(\"Loaded discriminator weights\", flush=True)\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "    \n",
    "    def generate_samples(self, num_sam, path):\n",
    "        sam_seed = tf.random.normal((num_sam, self.noise_latent_dim))\n",
    "        sam_pics = self.g(sam_seed)\n",
    "        for i in range(sam_pics.shape[0]):\n",
    "            if self.num_channel == 1:\n",
    "                plt.imshow(sam_pics[i,:,:,0], cmap='gray')\n",
    "            else:   \n",
    "                plt.imshow(tf.cast(tf.math.round(sam_pics[i,:,:,:]*127.5+127.5), tf.int32))\n",
    "            plt.axis('off')\n",
    "            plt.savefig(path+'/image_{:04d}.png'.format(i))\n",
    "            plt.close('all')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  \"/home/tony/TO_BE_REMOVED/mnist_ds/mnist_jpg/training\",\n",
    "  seed=123,\n",
    "  image_size=(32, 32),\n",
    "  batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"/home/tony/TO_BE_REMOVED/mnist_ds/mnist_jpg/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "dcgan1 = DCGAN(ds_path, (32, 32), 1, 25, disc_update_multi=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Generator Avg. Loss:  41.3724 , Discriminator Avg. Loss:  -29.27722\n",
      "Epoch 0001 Generator Avg. Loss:  37.15732 , Discriminator Avg. Loss:  -25.44714\n",
      "Epoch 0002 Generator Avg. Loss:  35.263073 , Discriminator Avg. Loss:  -23.928146\n",
      "Epoch 0003 Generator Avg. Loss:  27.913044 , Discriminator Avg. Loss:  -17.721321\n",
      "Epoch 0004 Generator Avg. Loss:  22.781689 , Discriminator Avg. Loss:  -12.914641\n",
      "Epoch 0005 Generator Avg. Loss:  20.530334 , Discriminator Avg. Loss:  -10.875239\n",
      "Epoch 0006 Generator Avg. Loss:  19.455324 , Discriminator Avg. Loss:  -9.466028\n",
      "Epoch 0007 Generator Avg. Loss:  19.035072 , Discriminator Avg. Loss:  -8.864852\n",
      "Epoch 0008 Generator Avg. Loss:  18.171598 , Discriminator Avg. Loss:  -7.996022\n",
      "Epoch 0009 Generator Avg. Loss:  17.702473 , Discriminator Avg. Loss:  -7.164987\n",
      "Epoch 0010 Generator Avg. Loss:  17.294514 , Discriminator Avg. Loss:  -6.5633073\n",
      "Epoch 0011 Generator Avg. Loss:  16.996038 , Discriminator Avg. Loss:  -6.0955524\n",
      "Epoch 0012 Generator Avg. Loss:  16.996815 , Discriminator Avg. Loss:  -5.746956\n",
      "Epoch 0013 Generator Avg. Loss:  17.148281 , Discriminator Avg. Loss:  -5.541143\n",
      "Epoch 0014 Generator Avg. Loss:  17.14663 , Discriminator Avg. Loss:  -5.3158183\n",
      "Epoch 0015 Generator Avg. Loss:  16.987759 , Discriminator Avg. Loss:  -5.1239104\n",
      "Epoch 0016 Generator Avg. Loss:  16.953785 , Discriminator Avg. Loss:  -4.976148\n",
      "Epoch 0017 Generator Avg. Loss:  16.926336 , Discriminator Avg. Loss:  -4.856981\n",
      "Epoch 0018 Generator Avg. Loss:  17.006308 , Discriminator Avg. Loss:  -4.7474747\n",
      "Epoch 0019 Generator Avg. Loss:  17.132235 , Discriminator Avg. Loss:  -4.5971856\n",
      "Epoch 0020 Generator Avg. Loss:  17.084446 , Discriminator Avg. Loss:  -4.4969673\n",
      "Epoch 0021 Generator Avg. Loss:  16.930319 , Discriminator Avg. Loss:  -4.374988\n",
      "Epoch 0022 Generator Avg. Loss:  17.066727 , Discriminator Avg. Loss:  -4.28238\n",
      "Epoch 0023 Generator Avg. Loss:  17.195108 , Discriminator Avg. Loss:  -4.2152786\n",
      "Epoch 0024 Generator Avg. Loss:  17.210989 , Discriminator Avg. Loss:  -4.17491\n",
      "Saved generator weights\n",
      "Saved discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan1.train(25,'./imgs')\n",
    "dcgan1.save_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator weights\n",
      "Loaded discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan1.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan1.generate_samples(10, './samples/0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "dcgan2 = DCGAN(ds_path, (32, 32), 1, 25, disc_update_multi=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator weights\n",
      "Loaded discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan2.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Generator Avg. Loss:  17.493753 , Discriminator Avg. Loss:  -4.0820417\n",
      "Epoch 0001 Generator Avg. Loss:  17.52289 , Discriminator Avg. Loss:  -4.0324664\n",
      "Epoch 0002 Generator Avg. Loss:  17.427225 , Discriminator Avg. Loss:  -3.9612927\n",
      "Epoch 0003 Generator Avg. Loss:  17.620392 , Discriminator Avg. Loss:  -3.9564266\n",
      "Epoch 0004 Generator Avg. Loss:  17.537416 , Discriminator Avg. Loss:  -3.9235697\n",
      "Epoch 0005 Generator Avg. Loss:  17.557526 , Discriminator Avg. Loss:  -3.8974373\n",
      "Epoch 0006 Generator Avg. Loss:  17.472876 , Discriminator Avg. Loss:  -3.8517308\n",
      "Epoch 0007 Generator Avg. Loss:  17.587835 , Discriminator Avg. Loss:  -3.8309057\n",
      "Epoch 0008 Generator Avg. Loss:  17.412464 , Discriminator Avg. Loss:  -3.783033\n",
      "Epoch 0009 Generator Avg. Loss:  17.378683 , Discriminator Avg. Loss:  -3.7449489\n",
      "Epoch 0010 Generator Avg. Loss:  17.583921 , Discriminator Avg. Loss:  -3.7130334\n",
      "Epoch 0011 Generator Avg. Loss:  17.739964 , Discriminator Avg. Loss:  -3.6982608\n",
      "Epoch 0012 Generator Avg. Loss:  17.80226 , Discriminator Avg. Loss:  -3.7022007\n",
      "Epoch 0013 Generator Avg. Loss:  17.841642 , Discriminator Avg. Loss:  -3.6430047\n",
      "Epoch 0014 Generator Avg. Loss:  17.921312 , Discriminator Avg. Loss:  -3.6193402\n",
      "Epoch 0015 Generator Avg. Loss:  17.737324 , Discriminator Avg. Loss:  -3.580725\n",
      "Epoch 0016 Generator Avg. Loss:  17.698833 , Discriminator Avg. Loss:  -3.562964\n",
      "Epoch 0017 Generator Avg. Loss:  17.920725 , Discriminator Avg. Loss:  -3.5215902\n",
      "Epoch 0018 Generator Avg. Loss:  18.044903 , Discriminator Avg. Loss:  -3.489955\n",
      "Epoch 0019 Generator Avg. Loss:  18.224966 , Discriminator Avg. Loss:  -3.494044\n",
      "Epoch 0020 Generator Avg. Loss:  18.183466 , Discriminator Avg. Loss:  -3.4771914\n",
      "Epoch 0021 Generator Avg. Loss:  17.979448 , Discriminator Avg. Loss:  -3.4457421\n",
      "Epoch 0022 Generator Avg. Loss:  18.059792 , Discriminator Avg. Loss:  -3.3918955\n",
      "Epoch 0023 Generator Avg. Loss:  18.149412 , Discriminator Avg. Loss:  -3.3795707\n",
      "Epoch 0024 Generator Avg. Loss:  18.1743 , Discriminator Avg. Loss:  -3.3782675\n",
      "Epoch 0025 Generator Avg. Loss:  18.22996 , Discriminator Avg. Loss:  -3.347092\n",
      "Epoch 0026 Generator Avg. Loss:  18.310095 , Discriminator Avg. Loss:  -3.3694994\n",
      "Epoch 0027 Generator Avg. Loss:  18.221605 , Discriminator Avg. Loss:  -3.353822\n",
      "Epoch 0028 Generator Avg. Loss:  18.194239 , Discriminator Avg. Loss:  -3.3063762\n",
      "Epoch 0029 Generator Avg. Loss:  18.340706 , Discriminator Avg. Loss:  -3.309414\n",
      "Epoch 0030 Generator Avg. Loss:  18.261969 , Discriminator Avg. Loss:  -3.2691295\n",
      "Epoch 0031 Generator Avg. Loss:  18.360935 , Discriminator Avg. Loss:  -3.2272584\n",
      "Epoch 0032 Generator Avg. Loss:  18.380833 , Discriminator Avg. Loss:  -3.2385228\n",
      "Epoch 0033 Generator Avg. Loss:  18.512836 , Discriminator Avg. Loss:  -3.2101557\n",
      "Epoch 0034 Generator Avg. Loss:  18.458017 , Discriminator Avg. Loss:  -3.199431\n",
      "Epoch 0035 Generator Avg. Loss:  18.427801 , Discriminator Avg. Loss:  -3.2045166\n",
      "Epoch 0036 Generator Avg. Loss:  18.376558 , Discriminator Avg. Loss:  -3.2068145\n",
      "Epoch 0037 Generator Avg. Loss:  18.293936 , Discriminator Avg. Loss:  -3.170817\n",
      "Epoch 0038 Generator Avg. Loss:  18.430584 , Discriminator Avg. Loss:  -3.1460009\n",
      "Epoch 0039 Generator Avg. Loss:  18.539785 , Discriminator Avg. Loss:  -3.1542168\n",
      "Epoch 0040 Generator Avg. Loss:  18.60109 , Discriminator Avg. Loss:  -3.1640518\n",
      "Epoch 0041 Generator Avg. Loss:  18.467474 , Discriminator Avg. Loss:  -3.1133544\n",
      "Epoch 0042 Generator Avg. Loss:  18.645123 , Discriminator Avg. Loss:  -3.0796838\n",
      "Epoch 0043 Generator Avg. Loss:  18.494333 , Discriminator Avg. Loss:  -3.0770981\n",
      "Epoch 0044 Generator Avg. Loss:  18.62055 , Discriminator Avg. Loss:  -3.0522118\n",
      "Epoch 0045 Generator Avg. Loss:  18.55007 , Discriminator Avg. Loss:  -3.049743\n",
      "Epoch 0046 Generator Avg. Loss:  18.612469 , Discriminator Avg. Loss:  -3.0464294\n",
      "Epoch 0047 Generator Avg. Loss:  18.596092 , Discriminator Avg. Loss:  -3.02811\n",
      "Epoch 0048 Generator Avg. Loss:  18.49043 , Discriminator Avg. Loss:  -3.0069268\n",
      "Epoch 0049 Generator Avg. Loss:  18.477345 , Discriminator Avg. Loss:  -2.9995632\n"
     ]
    }
   ],
   "source": [
    "dcgan2.train(50, './imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator weights\n",
      "Saved discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan2.save_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator weights\n",
      "Loaded discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan1.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan1.generate_samples(10, './samples/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_celeba_path = \"/home/tony/TO_BE_REMOVED/celeba_7z_alligned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dcgan3 = DCGAN(ds_celeba_path, (64, 64), 3, 100, disc_update_multi=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dcgan3.train(1,'./imgs')\n",
    "dcgan3.save_weights('./weights/g_celeb_test.ckpt', './weights/d_celeb_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
