{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: tensorflow in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow_addons in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: tensorflow_datasets in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (4.6.0)\n",
      "Requirement already satisfied: imageio in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (2.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (1.23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from matplotlib) (4.36.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: promise in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (2.28.1)\n",
      "Requirement already satisfied: toml in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (4.64.0)\n",
      "Requirement already satisfied: dill in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (0.3.5.1)\n",
      "Requirement already satisfied: etils[epath] in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (0.7.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow_datasets) (1.9.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.11)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: zipp in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from etils[epath]->tensorflow_datasets) (3.8.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/anthonylaw/anaconda3/envs/gan/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib tensorflow tensorflow_addons tensorflow_datasets imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 13:44:27.121007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#   \"/home/tony/TO_BE_REMOVED/celeba_data/imgs/\",\n",
    "  \"/Users/anthonylaw/Desktop/Endless/GAN-devel/mnist_ds/mnist_jpg/training\",\n",
    "  seed=123,\n",
    "  image_size=(32, 32),\n",
    "  batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i_b, l_b in train_ds:\n",
    "#     print(i_b.shape)\n",
    "#     print(tf.image.rgb_to_grayscale(i_b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images should be normalized to [-1,1] ***(Done in Arch)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the network size for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "\n",
    "    def __init__(self, noise_dim, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.noise_dim = noise_dim\n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = int(min(max(min(image_shape[0]/8.0-3.0, image_shape[1]/8.0-3.0), 3.0), 9.0))\n",
    "\n",
    "        self.lr_d = layers.ReLU()\n",
    "        self.lr_c1 = layers.ReLU()\n",
    "        self.lr_c2 = layers.ReLU()\n",
    "        self.lr_c3 = layers.ReLU()\n",
    "        \n",
    "        self.init_dense = layers.Dense(image_shape[0]/8.0*image_shape[1]/8.0*64,\n",
    "                               use_bias=False, input_shape=(self.noise_dim,))\n",
    "        \n",
    "        self.init_reshape = layers.Reshape((int(image_shape[0]/8.0), int(image_shape[1]/8.0), 64))\n",
    "        \n",
    "        self.conv2dT1 = layers.Conv2DTranspose(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(1, 1), padding='same')\n",
    "        self.conv2dT2 = layers.Conv2DTranspose(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2dT3 = layers.Conv2DTranspose(16, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2dTactv = layers.Conv2DTranspose(self.num_channel, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same', activation='tanh')\n",
    "\n",
    "    def call(self, noise_vec):\n",
    "\n",
    "        init_vec = tf.squeeze(self.lr_d(self.init_dense(noise_vec)))\n",
    "        \n",
    "#         print(init_vec.shape)\n",
    "        \n",
    "        reshaped = self.init_reshape(init_vec)\n",
    "        \n",
    "#         print(reshaped.shape)\n",
    "        \n",
    "        convt1 = self.lr_c1(self.conv2dT1(reshaped))\n",
    "\n",
    "#         print(convt1.shape)\n",
    "        \n",
    "        convt2 = self.lr_c2(self.conv2dT2(convt1))\n",
    "        \n",
    "#         print(convt2.shape)\n",
    "                         \n",
    "        convt3 = self.lr_c3(self.conv2dT3(convt2))\n",
    "        \n",
    "#         print(convt3.shape)\n",
    "            \n",
    "        out = self.conv2dTactv(convt3)\n",
    "        \n",
    "#         print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = Generator(10, (32, 32), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n",
      "(5, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "noise_input = tf.random.normal((5, 10))\n",
    "print(noise_input.shape)\n",
    "pics1 = g1(tf.expand_dims(noise_input, 0))\n",
    "print(pics1.shape)\n",
    "# plt.imshow(pics1[-1, :, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " re_lu (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              multiple                  0         \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              multiple                  0         \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  10240     \n",
      "                                                                 \n",
      " reshape (Reshape)           multiple                  0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  multiple                 36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  multiple                 18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  multiple                 4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  multiple                 435       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,691\n",
      "Trainable params: 70,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "\n",
    "    def __init__(self, image_shape, num_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.kernel_size = int(min(max(min(image_shape[0]/8.0-3.0, image_shape[1]/8.0-3.0), 3.0), 9.0))\n",
    "\n",
    "        self.lr_c1 = layers.LeakyReLU()\n",
    "        self.lr_c2 = layers.LeakyReLU()\n",
    "        self.lr_c3 = layers.LeakyReLU()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.conv2d1 = layers.Conv2D(16, (self.kernel_size, self.kernel_size),\n",
    "                                        strides=(2, 2), padding='same',\n",
    "                                        input_shape=(None, self.image_shape[0],\n",
    "                                        self.image_shape[1], self.num_channel))\n",
    "        self.conv2d2 = layers.Conv2D(32, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.conv2d3 = layers.Conv2D(64, (self.kernel_size, self.kernel_size),\n",
    "                                               strides=(2, 2), padding='same')\n",
    "        self.dense_actv = layers.Dense(64,\n",
    "                                      )\n",
    "#                                        activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, img_input):\n",
    "        \n",
    "        conv1 = self.lr_c1(self.conv2d1(img_input))\n",
    "        \n",
    "#         print(conv1.shape)\n",
    "        \n",
    "        conv2 = self.lr_c2(self.conv2d2(conv1))\n",
    "        \n",
    "#         print(conv2.shape)\n",
    "                         \n",
    "        conv3 = self.lr_c3(self.conv2d3(conv2))\n",
    "        \n",
    "#         print(conv3.shape)\n",
    "        \n",
    "        flat = self.flatten(conv3)\n",
    "        \n",
    "#         print(flat.shape)\n",
    "        \n",
    "        out = tf.squeeze(self.dense_actv(flat))\n",
    "        \n",
    "#         print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = Discriminator((32,32), 3)\n",
    "g2 = Generator(10, (32,32), 3)\n",
    "d1.kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "noise_input = tf.random.normal((5, 10))\n",
    "pics2 = g1(tf.expand_dims(noise_input, 0))\n",
    "# plt.imshow(pics2[-1, :, :, 0], cmap='gray')\n",
    "print(pics2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 64), dtype=float32, numpy=\n",
       "array([[ 6.85174833e-04, -1.97368488e-03, -1.51406229e-03,\n",
       "        -6.73288596e-04, -1.64995389e-03,  1.10560225e-03,\n",
       "         4.90562874e-04, -3.07230977e-04,  1.27579924e-03,\n",
       "        -1.04062399e-03, -7.33286259e-04,  6.21622952e-04,\n",
       "        -1.44893397e-03,  5.56204002e-04,  8.09336489e-05,\n",
       "         1.42288918e-05,  1.18794863e-03, -4.48025879e-04,\n",
       "         5.87049581e-04,  5.14148967e-04, -5.99993858e-04,\n",
       "        -7.24156387e-04, -8.08689860e-04, -1.25736557e-03,\n",
       "        -1.67869875e-04, -2.54387781e-03,  2.05661752e-03,\n",
       "         2.41507310e-03,  6.38983620e-04, -2.09220088e-04,\n",
       "         1.11503317e-03, -1.18578109e-03,  3.43304710e-04,\n",
       "         3.34557291e-04,  9.66406369e-04, -6.11025607e-05,\n",
       "         8.40763823e-05, -1.15453277e-03,  3.62648163e-04,\n",
       "        -9.63857980e-04, -2.89332820e-04, -1.27725059e-03,\n",
       "         5.05678690e-05,  1.43870187e-04,  2.10751407e-03,\n",
       "        -9.41049308e-04,  2.46559284e-05,  9.13130352e-05,\n",
       "         8.40886845e-04, -9.21169471e-04, -2.44017312e-04,\n",
       "        -3.22223466e-04, -2.28907389e-04,  1.09848741e-03,\n",
       "        -5.67791285e-04, -8.37107713e-04,  4.38040734e-04,\n",
       "        -1.19024748e-03,  8.70135147e-04, -1.23434677e-03,\n",
       "        -3.27084621e-04, -4.18313517e-04,  4.22443816e-04,\n",
       "         1.06323953e-03],\n",
       "       [ 4.97825502e-04, -1.56889693e-03, -2.66135647e-03,\n",
       "        -1.50585966e-03, -1.07899727e-03,  1.55160914e-03,\n",
       "        -2.13071180e-04, -1.26009632e-04,  1.54576194e-03,\n",
       "        -1.66378776e-03,  2.86750146e-05,  8.28752294e-04,\n",
       "        -7.91756087e-04,  2.59745144e-03, -1.68679529e-04,\n",
       "        -1.14923879e-03, -2.09886319e-04,  1.56558282e-03,\n",
       "         5.66126313e-04, -6.97555544e-04, -2.15516007e-03,\n",
       "        -2.30067439e-04, -7.49184634e-04, -2.08895421e-03,\n",
       "        -2.26695847e-04, -2.30363198e-03,  1.51618570e-03,\n",
       "         2.28296593e-03,  5.75074344e-04, -7.01233686e-04,\n",
       "         1.50897540e-06, -9.21554747e-04,  4.51439293e-04,\n",
       "        -3.15508863e-04,  6.14376040e-04,  1.75396015e-03,\n",
       "         2.63820519e-04,  5.05267759e-04,  2.17461726e-03,\n",
       "        -1.58812176e-03, -3.40452709e-04, -3.21873184e-03,\n",
       "        -4.76596993e-04,  1.18711684e-03, -9.64776613e-04,\n",
       "        -1.11113396e-03, -1.19631179e-03,  1.38401199e-04,\n",
       "         1.26601581e-03, -5.18734043e-04, -7.74156651e-05,\n",
       "        -7.34626956e-05, -3.72052426e-04,  1.46683946e-04,\n",
       "         4.02375008e-05, -8.19817244e-04,  8.26266943e-04,\n",
       "        -1.56641356e-03,  1.04964594e-04,  5.21167880e-04,\n",
       "        -1.22158439e-04, -3.52308183e-04,  4.40653908e-04,\n",
       "        -4.69202671e-04],\n",
       "       [-4.26535174e-04, -1.19403726e-03, -1.20336493e-03,\n",
       "        -1.09052332e-03, -1.10276544e-03,  1.36458117e-03,\n",
       "         7.76051136e-04,  8.07213539e-04,  1.67294522e-03,\n",
       "        -1.69683131e-03, -4.84603457e-04, -3.80401296e-04,\n",
       "        -1.09021028e-03,  1.17574725e-03, -1.29472392e-04,\n",
       "         4.37217357e-04, -7.63310818e-05,  4.24635364e-04,\n",
       "         4.39290743e-04,  5.45633491e-04, -2.38149217e-03,\n",
       "        -5.21587091e-04,  2.81797751e-04, -7.33274152e-04,\n",
       "         6.32989511e-04, -2.86699994e-03,  8.02834285e-04,\n",
       "         3.61408049e-04,  1.06533291e-03, -2.14883359e-04,\n",
       "         5.95791615e-04, -1.34117587e-03,  3.77887802e-04,\n",
       "        -1.16762414e-03, -1.48821040e-04, -8.53694975e-04,\n",
       "         5.00284717e-04,  2.55697261e-04,  1.28116808e-03,\n",
       "        -5.75182959e-04, -3.66376858e-04, -1.74222759e-03,\n",
       "        -4.79350856e-04, -1.23745442e-04, -1.08365981e-04,\n",
       "        -5.68925636e-04, -8.36111780e-04, -3.35673685e-05,\n",
       "         7.98079069e-04, -8.82777676e-04,  2.96578597e-04,\n",
       "         1.97234447e-03, -2.01384217e-04, -2.96119106e-04,\n",
       "        -1.43604761e-03,  4.42865537e-04, -2.83315516e-04,\n",
       "        -1.18209363e-03,  4.71674750e-04, -9.83686885e-04,\n",
       "        -9.39033867e-04, -1.01374695e-03, -1.10763081e-04,\n",
       "         4.06117062e-04],\n",
       "       [ 5.03334391e-04, -1.14895217e-03, -2.41439673e-03,\n",
       "        -1.33930170e-03, -2.15577078e-04,  3.39940918e-04,\n",
       "        -1.11101574e-04,  5.16854227e-04,  1.52965041e-03,\n",
       "        -9.47165769e-04,  2.82190682e-04,  1.55934162e-04,\n",
       "        -1.00617716e-03,  3.28627648e-04, -2.12239102e-04,\n",
       "         4.09939006e-04,  1.12902443e-03,  4.30373271e-04,\n",
       "         1.96333113e-03, -7.81571667e-04, -1.92218984e-03,\n",
       "        -1.10088964e-03, -4.32570203e-04, -5.74123696e-04,\n",
       "        -2.47147778e-04, -1.71093014e-03,  8.89189483e-04,\n",
       "         2.16826587e-03, -2.65734096e-04, -2.10161263e-04,\n",
       "         1.87284313e-04, -1.53141504e-04,  5.29103214e-04,\n",
       "        -9.81226913e-04,  8.52363475e-04, -3.34041310e-04,\n",
       "         9.13880125e-04, -9.38769663e-04,  4.91901592e-04,\n",
       "        -3.60446458e-04, -1.53273606e-04, -1.19073526e-03,\n",
       "        -7.28434010e-04,  8.00025708e-04,  3.47221212e-04,\n",
       "        -1.25039555e-03, -8.56494764e-04, -1.12126069e-03,\n",
       "         1.12918916e-03,  4.06949664e-04, -3.41630541e-04,\n",
       "        -1.45267602e-03, -7.88539415e-04,  9.04535060e-04,\n",
       "         9.88110114e-05, -3.36272467e-04,  8.86567053e-04,\n",
       "        -4.08648717e-04,  1.18349073e-03, -1.18796900e-03,\n",
       "        -5.97242150e-04, -8.71095050e-04,  1.13193586e-04,\n",
       "         6.75262476e-04],\n",
       "       [ 3.00152082e-04, -8.81930348e-04, -1.96536793e-03,\n",
       "         1.48139428e-04, -1.11749256e-03,  1.00831792e-03,\n",
       "        -2.15476757e-04, -4.04391263e-04,  3.31380143e-04,\n",
       "        -1.68801635e-03, -7.18105235e-04,  5.96958387e-04,\n",
       "         5.66327944e-05,  1.14992238e-03, -4.28615866e-04,\n",
       "         2.67063413e-04,  4.10439447e-04,  8.88921495e-05,\n",
       "         1.58448133e-03, -4.69071849e-04, -7.50731502e-04,\n",
       "        -1.48581131e-03, -9.54903255e-04, -2.51167035e-03,\n",
       "         6.28228765e-04, -2.62116501e-03,  8.03859206e-04,\n",
       "         2.23252620e-03, -2.83300527e-04, -1.14353909e-03,\n",
       "        -2.64491653e-04, -1.21029047e-03, -4.21058154e-04,\n",
       "        -5.36252162e-04,  1.65241351e-03, -2.27181197e-04,\n",
       "         8.16376298e-04, -4.54309076e-04,  1.15900068e-03,\n",
       "        -5.14459040e-04,  2.94902886e-04, -9.52288974e-04,\n",
       "        -2.21896596e-04,  5.88550873e-04,  7.53357308e-04,\n",
       "        -1.77108916e-04, -7.91185652e-04, -3.75970354e-04,\n",
       "         1.07401703e-03, -2.92681332e-04,  3.69028130e-05,\n",
       "         3.07011069e-05, -1.58550625e-04,  1.13577233e-03,\n",
       "        -6.41545223e-04,  6.01214415e-04,  6.73035625e-04,\n",
       "        -8.70214542e-04, -4.65287420e-04, -7.20946584e-04,\n",
       "        -3.16840771e-04,  3.46163986e-04,  8.28116259e-04,\n",
       "         4.60600480e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deci = d1(pics2)\n",
    "deci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " leaky_re_lu (LeakyReLU)     multiple                  0         \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   multiple                  0         \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   multiple                  0         \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             multiple                  448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  4640      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           multiple                  18496     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  65600     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,184\n",
      "Trainable params: 89,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CramerDCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-16\n",
    "\n",
    "class DCGAN:\n",
    "    \n",
    "    def __init__(self, dataset_path, image_shape, num_channel, noise_latent_dim,\n",
    "                 buffer_size=60000, batch_size=256, lr=3e-4, gp_lam = 10.0):\n",
    "        assert len(image_shape) == 2\n",
    "        assert image_shape[0]%8 == 0\n",
    "        assert image_shape[1]%8 == 0\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.num_channel = num_channel\n",
    "        self.noise_latent_dim = noise_latent_dim\n",
    "        self.buffer_cap, self.batch_size, self.gp_lam = buffer_size, batch_size, gp_lam\n",
    "        self.num_img_prog_monit = 16\n",
    "        \n",
    "        self.dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "                              dataset_path,\n",
    "                              seed=123,\n",
    "                              image_size=self.image_shape,\n",
    "                              batch_size=self.batch_size)\n",
    "        # NOTE: Dataset must be processed differently for different source and applications\n",
    "        \n",
    "        self.g = Generator(self.noise_latent_dim, self.image_shape, self.num_channel)\n",
    "        self.d = Discriminator(self.image_shape, self.num_channel)\n",
    "        \n",
    "        self.g_opt = tf.keras.optimizers.Adam(lr)\n",
    "        self.d_opt = tf.keras.optimizers.Adam(lr)\n",
    "        \n",
    "        self.g_seed = tf.random.normal((self.num_img_prog_monit, self.noise_latent_dim))\n",
    "\n",
    "    def cramer_loss(self, d_x_data, d_g_z_1, d_g_z_2, x_it):\n",
    "        \n",
    "        crit_r = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_x_data, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_x_data**2, axis = 1)+EPSILON))\n",
    "        crit_g_1 = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_g_z_1, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_g_z_1**2, axis = 1)+EPSILON))\n",
    "        \n",
    "        L_srg = tf.math.add(crit_r, -crit_g_1)\n",
    "        \n",
    "        with tf.GradientTape() as t_gp:\n",
    "            t_gp.watch(x_it)\n",
    "            d_it = self.d(x_it)\n",
    "            crit_it = tf.math.add(tf.math.sqrt(tf.reduce_sum(tf.math.add(d_it, -d_g_z_2)**2, axis = 1)+EPSILON),\n",
    "                   -tf.math.sqrt(tf.reduce_sum(d_it**2, axis = 1)+EPSILON))\n",
    "            \n",
    "        gp_grad = t_gp.gradient(crit_it, x_it)\n",
    "        l2n_gp = tf.math.sqrt(tf.reduce_sum(gp_grad**2, axis = [1,2,3])+EPSILON)\n",
    "        \n",
    "        # d_loss\n",
    "        L_d = tf.reduce_mean(-L_srg + (self.gp_lam*((l2n_gp-1.0)**2)))\n",
    "\n",
    "        l2nrg1 = tf.math.sqrt(tf.reduce_sum(tf.math.add(d_x_data, -d_g_z_1)**2, axis = 1)+EPSILON)\n",
    "        l2nrg2 = tf.math.sqrt(tf.reduce_sum(tf.math.add(d_x_data, -d_g_z_2)**2, axis = 1)+EPSILON)\n",
    "        l2ng12 = tf.math.sqrt(tf.reduce_sum(tf.math.add(d_g_z_1, -d_g_z_2)**2, axis = 1)+EPSILON)\n",
    "\n",
    "        # g_loss\n",
    "        L_g = tf.reduce_mean(l2nrg1 + l2nrg2 - l2ng12)\n",
    "\n",
    "        return L_g, L_d\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def update(self, imgs):\n",
    "        noise_input1 = tf.random.normal((imgs.shape[0], self.noise_latent_dim))\n",
    "        noise_input2 = tf.random.normal((imgs.shape[0], self.noise_latent_dim))\n",
    "        \n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            g_z_1 = self.g(noise_input1)\n",
    "            g_z_2 = self.g(noise_input2)\n",
    "            \n",
    "            d_x_data = self.d(imgs)\n",
    "            d_g_z_1 = self.d(g_z_1)\n",
    "            d_g_z_2 = self.d(g_z_2)\n",
    "            \n",
    "            epsi = tf.random.uniform([imgs.shape[0], 1, 1, 1], 0.0, 1.0)\n",
    "            x_it = tf.math.add(epsi*imgs, (1.0-epsi)*g_z_1)\n",
    "            g_loss, d_loss = self.cramer_loss(d_x_data, d_g_z_1, d_g_z_2, x_it)\n",
    "            \n",
    "        grad_g = g_tape.gradient(g_loss, self.g.trainable_variables)\n",
    "        grad_d = d_tape.gradient(d_loss, self.d.trainable_variables)\n",
    "        \n",
    "        self.g_opt.apply_gradients(zip(grad_g, self.g.trainable_variables))\n",
    "        self.d_opt.apply_gradients(zip(grad_d, self.d.trainable_variables))\n",
    "        return g_loss, d_loss\n",
    "        \n",
    "    def train(self, epochs=50, train_moni_path=None):\n",
    "        for epo in range(epochs):\n",
    "            for img_b, l_b in self.dataset:\n",
    "                if self.num_channel == 1 and img_b.shape[-1] == 3:\n",
    "                    img_b = tf.image.rgb_to_grayscale(img_b)\n",
    "                norm_img_b = (img_b-127.5)/127.5\n",
    "                g_l, d_l = self.update(norm_img_b)\n",
    "            print(\"Generator Loss: \", g_l, \", Discriminator Loss: \",  d_l)\n",
    "            \n",
    "            if not train_moni_path == None:\n",
    "                self.monitor_progress(epo, train_moni_path)\n",
    "            \n",
    "    def monitor_progress(self, epo, path):\n",
    "        pics = self.g(self.g_seed)\n",
    "        \n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        for i in range(pics.shape[0]):\n",
    "            plt.subplot(4,4,i+1)\n",
    "            if self.num_channel == 1:\n",
    "                plt.imshow(pics[i,:,:,0], cmap='gray')\n",
    "            else:   \n",
    "                plt.imshow(tf.cast(tf.math.round(pics[i,:,:,:]*127.5+127.5), tf.int32))\n",
    "            plt.axis('off')\n",
    "            \n",
    "        plt.savefig(path+'/image_{:04d}.png'.format(epo))\n",
    "#         plt.savefig('/home/tony/TO_BE_REMOVED/imgs/image_{:04d}.png'.format(epo))\n",
    "        # NEEDS to be changed for machines\n",
    "        \n",
    "        plt.close('all')\n",
    "        \n",
    "    def save_weights(self, g_path, d_path):\n",
    "        self.g.save_weights(g_path)\n",
    "        print(\"Saved generator weights\", flush=True)\n",
    "        self.d.save_weights(d_path)\n",
    "        print(\"Saved discriminator weights\", flush=True)\n",
    "    def load_weights(self, g_path, d_path):\n",
    "        try:\n",
    "            self.g.load_weights(g_path)\n",
    "            print(\"Loaded generator weights\", flush=True)\n",
    "            self.d.load_weights(d_path)\n",
    "            print(\"Loaded discriminator weights\", flush=True)\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "    \n",
    "    def generate_samples(self, num_sam, path):\n",
    "        sam_seed = tf.random.normal((num_sam, self.noise_latent_dim))\n",
    "        sam_pics = self.g(sam_seed)\n",
    "        for i in range(sam_pics.shape[0]):\n",
    "            if self.num_channel == 1:\n",
    "                plt.imshow(sam_pics[i,:,:,0], cmap='gray')\n",
    "            else:   \n",
    "                plt.imshow(tf.cast(tf.math.round(pics[i,:,:,:]*127.5+127.5), tf.int32))\n",
    "            plt.axis('off')\n",
    "            plt.savefig(path+'/image_{:04d}.png'.format(i))\n",
    "            plt.close('all')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  \"/Users/anthonylaw/Desktop/Endless/GAN-devel/mnist_ds/mnist_jpg/training\",\n",
    "  seed=123,\n",
    "  image_size=(32, 32),\n",
    "  batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"/Users/anthonylaw/Desktop/Endless/GAN-devel/mnist_ds/mnist_jpg/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "dcgan1 = DCGAN(ds_path, (32, 32), 1, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss:  tf.Tensor(20.169584, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-13.992974, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(11.07236, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-6.241216, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(11.480617, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-6.047583, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(10.570411, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-4.2436476, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(10.378007, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-4.3242736, shape=(), dtype=float32)\n",
      "Saved generator weights\n",
      "Saved discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan1.train(5,'./imgs')\n",
    "dcgan1.save_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator weights\n",
      "Loaded discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan1.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan1.generate_samples(10, './samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "dcgan2 = DCGAN(ds_path, (32, 32), 1, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator weights\n",
      "Loaded discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan2.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss:  tf.Tensor(11.761238, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-4.6782985, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(10.948012, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-3.9494812, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(9.988838, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-3.4645674, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(11.518083, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-2.2646255, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(10.231049, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-2.9900744, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(10.958645, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-2.9179451, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(10.184042, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-2.4785311, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(9.687585, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-2.1655881, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(10.787792, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-2.4813194, shape=(), dtype=float32)\n",
      "Generator Loss:  tf.Tensor(9.541611, shape=(), dtype=float32) , Discriminator Loss:  tf.Tensor(-1.9322023, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dcgan2.train(10, './imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generator weights\n",
      "Saved discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan2.save_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded generator weights\n",
      "Loaded discriminator weights\n"
     ]
    }
   ],
   "source": [
    "dcgan1.load_weights('./weights/g_test.ckpt', './weights/d_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan1.generate_samples(10, './samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
